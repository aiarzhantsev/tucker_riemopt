{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from src.optimize import RGD, RCG, OptimizationConfig\n",
    "from src.tucker import Tucker\n",
    "from src.matrix import TuckerMatrix\n",
    "\n",
    "from src import backend as back\n",
    "from src import set_backend\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Following examples illustrates calculation of minimal eigenvalue of the\n",
    "symmetric positive defined matrix.\n",
    "\n",
    "Let $R(x) = \\frac{\\langle x, Ax\\rangle}{\\|x\\|^2}$ be Rayleigh quotient,\n",
    "then $\\min\\limits_{x} R(x)$ is exactly minimal eigenvalue of self adjoint\n",
    "linear operator $A$.\n",
    "\n",
    "Let us represent $A$ as self adjoint linear operator on linear space\n",
    "$\\mathbb{R}^{n \\times \\ldots \\times n}$. Then we might want to solve the\n",
    "following problem\n",
    "\n",
    "\\begin{align*}\n",
    "    \\min\\limits_{x \\in \\mathcal{M}_{\\textbf{r}}} R(x).\n",
    "\\end{align*}\n",
    "\n",
    "Where by $\\mathcal{M}_{\\textbf{r}}$ we denote Riemann submanifold of tensors\n",
    "of fixed multilinear rank. So let us launch basic Riemann steepest descent to\n",
    "find the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-0a7c279157ca>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTucker\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfull2tuck\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m config = OptimizationConfig({\n\u001B[0m\u001B[1;32m     27\u001B[0m     \u001B[0;34m\"method_trace\"\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;34m\"tolerance\"\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0;36m1e-8\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    A: operator on space R^{10 x 10}\n",
    "    X: tensor from space R^{10 x 10}\n",
    "\"\"\"\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "def R(x: Tucker):\n",
    "    return (x.flat_inner(A @ x)) / x.flat_inner(x)\n",
    "\n",
    "n = 100\n",
    "k = np.sqrt(n).astype(int)\n",
    "A = back.tensor(np.random.randint(1, 11, n))\n",
    "exact_solution = A.min()\n",
    "A = back.diag(A)\n",
    "Q = back.qr(back.randn((n, n), random_seed))[0]\n",
    "A = Q @ A @ Q.T\n",
    "A = TuckerMatrix.full2tuck(A.reshape([k] * 4),\n",
    "                           [k] * 2, [k] * 2)\n",
    "\n",
    "X = back.randn((n,))\n",
    "X = Tucker.full2tuck(X.reshape([k] * 2))\n",
    "\n",
    "config = OptimizationConfig({\n",
    "    \"method_trace\" : True,\n",
    "    \"tolerance\" : 1e-8\n",
    "})\n",
    "start = time()\n",
    "X, history = RGD(R, X, config)\n",
    "\n",
    "print(f\"Exact solution is \\t{exact_solution}\")\n",
    "print(\"Found solution is \\t%.6f\" % R(X))\n",
    "print(\"It took %.3f seconds and %d iterations to converge\" % (time() - start, len(history[\"func\"])))\n",
    "print(\"||Ax - λx|| = %.6f\" % (A @ X - R(X) * X).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(ncols=2, nrows=1, figsize=(28, 9))\n",
    "ax[0].set(title=\"Riemann gradient norm\", xlabel=\"iterations\", ylabel=r\"$\\|\\mathrm{grad} f\\|$\")\n",
    "ax[1].set(title=\"Residuals for eigenvalue\", xlabel=\"iterations\", ylabel=r\"$R(x) - \\lambda_{\\min}$\")\n",
    "\n",
    "\n",
    "ax[0].semilogy(np.arange(len(history[\"func\"])), history[\"func\"])\n",
    "ax[1].semilogy(np.arange(len(history[\"grad_norm\"])), history[\"grad_norm\"])\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riemann steepest descent succsessfuly converges to minimal eigenvalue of our tensor operator. Howerever found tensor $X$ kinda far from corresponding eigenvector in terms of Frobenius norm.\n",
    "\n",
    "Now let us launch more accurate optimization method: Rieman conjugate gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = back.randn((n,))\n",
    "X = Tucker.full2tuck(X.reshape([k] * 2))\n",
    "\n",
    "config = OptimizationConfig({\n",
    "    \"method_trace\" : True,\n",
    "    \"tolerance\" : 1e-8\n",
    "})\n",
    "start = time()\n",
    "X, history = RCG(R, X, config)\n",
    "\n",
    "print(f\"Exact solution is \\t{exact_solution}\")\n",
    "print(\"Found solution is \\t%.6f\" % R(X))\n",
    "print(\"It took %.3f seconds and %d iterations to converge\" % (time() - start, len(history[\"func\"])))\n",
    "print(\"||Ax - λx|| = %.6f\" % (A @ X - R(X) * X).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(ncols=2, nrows=1, figsize=(28, 9))\n",
    "ax[0].set(title=\"Riemann gradient norm\", xlabel=\"iterations\", ylabel=r\"$\\|\\mathrm{grad} f\\|$\")\n",
    "ax[1].set(title=\"Residuals for eigenvalue\", xlabel=\"iterations\", ylabel=r\"$R(x) - \\lambda_{\\min}$\")\n",
    "\n",
    "\n",
    "ax[0].semilogy(np.arange(len(history[\"func\"])), history[\"func\"])\n",
    "ax[1].semilogy(np.arange(len(history[\"grad_norm\"])), history[\"grad_norm\"])\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (local_collab)",
   "language": "python",
   "name": "pycharm-fe7f938c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}