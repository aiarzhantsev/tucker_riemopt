{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from src.optimize import RGD, RCG\n",
    "from src.tucker import Tucker\n",
    "from src.matrix import TuckerMatrix\n",
    "\n",
    "from src import backend as back\n",
    "from src import set_backend\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Following examples illustrates calculation of minimal eigenvalue of the\n",
    "symmetric positive defined matrix.\n",
    "\n",
    "Let $R(x) = \\frac{\\langle x, Ax\\rangle}{\\|x\\|^2}$ be Rayleigh quotient,\n",
    "then $\\min\\limits_{x} R(x)$ is exactly minimal eigenvalue of self adjoint\n",
    "linear operator $A$.\n",
    "\n",
    "Let us represent $A$ as self adjoint linear operator on linear space\n",
    "$\\mathbb{R}^{n \\times \\ldots \\times n}$. Then we might want to solve the\n",
    "following problem\n",
    "\n",
    "\\begin{align*}\n",
    "    \\min\\limits_{x \\in \\mathcal{M}_{\\textbf{r}}} R(x).\n",
    "\\end{align*}\n",
    "\n",
    "Where by $\\mathcal{M}_{\\textbf{r}}$ we denote Riemann submanifold of tensors\n",
    "of fixed multilinear rank. So let us launch basic Riemann steepest descent to\n",
    "find the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact solution is \t1\n",
      "Found solution is \t1.000000\n",
      "It took 6.341 seconds and 72 iterations to converge\n",
      "||Ax - λx|| = 0.000001\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    A: operator on space R^{10 x 10}\n",
    "    X: tensor from space R^{10 x 10}\n",
    "\"\"\"\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "def R(x: Tucker):\n",
    "    return (x.flat_inner(A @ x)) / x.flat_inner(x)\n",
    "\n",
    "n = 100\n",
    "k = np.sqrt(n).astype(int)\n",
    "A = back.tensor(np.random.randint(1, 11, n))\n",
    "exact_solution = A.min()\n",
    "A = back.diag(A)\n",
    "Q = back.qr(back.randn((n, n), random_seed))[0]\n",
    "A = Q @ A @ Q.T\n",
    "A = TuckerMatrix.full2tuck(A.reshape([k] * 4),\n",
    "                           [k] * 2, [k] * 2)\n",
    "\n",
    "X = back.randn((n,))\n",
    "X = Tucker.full2tuck(X.reshape([k] * 2))\n",
    "\n",
    "start = time()\n",
    "X, history = RGD(R, X, tolerance=1e-8, trace=True)\n",
    "\n",
    "print(f\"Exact solution is \\t{exact_solution}\")\n",
    "print(\"Found solution is \\t%.6f\" % R(X))\n",
    "print(\"It took %.3f seconds and %d iterations to converge\" % (time() - start, len(history[\"func\"])))\n",
    "print(\"||Ax - λx|| = %.6f\" % (A @ X - R(X) * X).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(ncols=2, nrows=1, figsize=(28, 9))\n",
    "ax[0].set(title=\"Riemann gradient norm\", xlabel=\"iterations\", ylabel=r\"$\\|\\mathrm{grad} f\\|$\")\n",
    "ax[1].set(title=\"Residuals for eigenvalue\", xlabel=\"iterations\", ylabel=r\"$R(x) - \\lambda_{\\min}$\")\n",
    "\n",
    "\n",
    "ax[0].semilogy(np.arange(len(history[\"func\"])), history[\"func\"])\n",
    "ax[1].semilogy(np.arange(len(history[\"grad_norm\"])), history[\"grad_norm\"])\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riemann steepest descent succsessfuly converges to minimal eigenvalue of our tensor operator. Howerever found tensor $X$ kinda far from corresponding eigenvector in terms of Frobenius norm.\n",
    "\n",
    "Now let us launch more accurate optimization method: Rieman conjugate gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = back.randn((n,))\n",
    "X = Tucker.full2tuck(X.reshape([k] * 2))\n",
    "\n",
    "start = time()\n",
    "X, history = RCG(R, X, tolerance=1e-8, trace=True)\n",
    "\n",
    "print(f\"Exact solution is \\t{exact_solution}\")\n",
    "print(\"Found solution is \\t%.6f\" % R(X))\n",
    "print(\"It took %.3f seconds and %d iterations to converge\" % (time() - start, len(history[\"func\"])))\n",
    "print(\"||Ax - λx|| = %.6f\" % (A @ X - R(X) * X).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(ncols=2, nrows=1, figsize=(28, 9))\n",
    "ax[0].set(title=\"Riemann gradient norm\", xlabel=\"iterations\", ylabel=r\"$\\|\\mathrm{grad} f\\|$\")\n",
    "ax[1].set(title=\"Residuals for eigenvalue\", xlabel=\"iterations\", ylabel=r\"$R(x) - \\lambda_{\\min}$\")\n",
    "\n",
    "\n",
    "ax[0].semilogy(np.arange(len(history[\"func\"])), history[\"func\"])\n",
    "ax[1].semilogy(np.arange(len(history[\"grad_norm\"])), history[\"grad_norm\"])\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (local_collab)",
   "language": "python",
   "name": "pycharm-fe7f938c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
